# -*- coding: utf-8 -*-
"""Handin_Happiness Score.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NS4mbatkm0J7Eb1kTdN0vz8SbivPEcgk

# **First things first**
It is imperative to call libraries which are needed to analyze and work with the dataset.
"""

#Necessary Libraries
import pandas as pd 
import numpy as np  #For mathematical calculatons
import seaborn as sns #For data visualization
import matplotlib.pyplot as plt # For plotting graphs
import plotly.graph_objs as go
import warnings      #To ignore warnings
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
import graphviz
from sklearn.metrics import accuracy_score
from sklearn import preprocessing
from sklearn import utils
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN
from sklearn.metrics import adjusted_rand_score
from sklearn.metrics import silhouette_score
warnings.filterwarnings ("ignore")

"""Connecting to Google Drive"""

#Conecting to GDrive
from google.colab import drive

drive.mount("/content/gdrive")  
!pwd

# Commented out IPython magic to ensure Python compatibility.
# %cd "/content/gdrive/MyDrive/Intelligent Systems"
!ls

"""### **Analyzing missing values and some useful information from the dataset.**"""

df = pd.read_csv('Happiness_Score.csv')
df.info()   #General information about dataset

df.isnull().sum()   #Calculating missing values from dataset

"""Using Spearman correlation matrix to see how much relaion resides between each possible pair of classes."""

spearman_cormatrix= df.corr(method='spearman')    #Defining Spearman Correlation Matrix
spearman_cormatrix

"""Then it is plotted:"""

plt.figure(figsize=(18,10))
ax = sns.heatmap(spearman_cormatrix, vmin=-1, vmax=1, 
            center=0, cmap="viridis", annot=True)
bottom, top = ax.get_ylim()
ax.set_ylim(bottom + 0.5, top - 0.5);
plt.title("Matrix of Spearman Correlation");

"""As per this excercise, the focus stays on Hapiness Score, so the column from that class is retreived:"""

spearman_cormatrix['happiness_score'].sort_values(ascending=False)

"""Now, it is important to see how the Happiness Score behaves in different regions of the world. So, a Ladder Score Distribution graph is presented:"""

plt.figure(figsize = (15,8))
sns.kdeplot(df["happiness_score"], hue = df["continent"], fill = True, linewidth = 2)
plt.axvline(df["happiness_score"].mean(), c = "black")
plt.title("Ladder Score Distribution")
plt.show()

"""Some cool animation of the Happiness Score from 2015-2020 arround the world:"""

import plotly.express as px
fig = px.choropleth(df.sort_values("Year"),
                   locations= 'Country',
                   color = "happiness_score",
                   locationmode = 'country names',
                    animation_frame = 'Year'
                   )
fig.update_layout(title= "Happiness Score Comprasion by Countries")

"""*Preparing data for Linear Regression by eliminating some columns that are not very useful:*"""

X = df.drop(["happiness_score","Country","continent","Year","generosity","family"], axis=1)
y = df["happiness_score"]   #Separating predictors from response variable
X.head()

"""*Rearrangement by the most influential classes from previous observations:*"""

column_names = ["gdp_per_capita", "health", "cpi_score","freedom","government_trust","social_support","dystopia_residual"]    #New arrangement of classes
X = df.reindex(columns=column_names)
X.head()

"""Then, one very important thing is to scale data for future usability:"""

from sklearn.preprocessing import MinMaxScaler, PolynomialFeatures, LabelEncoder
scale = MinMaxScaler()
X_scaled = scale.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled, columns=X.columns)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)    #Splitting train and test sets

"""## **Random Forest Implementation**

First, a Desicion Tree is established to see how large it is, and also to see how well performs:
"""

regressor = DecisionTreeRegressor(random_state = 0)
regressor.fit(X_train, y_train)
y_Tree_pred = regressor.predict(X_test)
r2_score(y_test,y_Tree_pred)

dot = tree.export_graphviz(regressor, out_file=None, 
                           feature_names=["gdp_per_capita", "health", "cpi_score","freedom","government_trust","social_support","dystopia_residual"][:],
                           class_names=None,
                           filled=True, rounded=True,  
                           special_characters=True) 
graph = graphviz.Source(dot) 
graph

RFregress = RandomForestRegressor()
RFregress.fit(X_train, y_train)
y_test_pred = RFregress.predict(X_test)
print('Testing accuracy on all features: %.3f' % r2_score(y_test, y_test_pred))

print("\nPredict your actual happiness based on metrics from your country")

predict = "Y"

while predict == "Y" or predict == "y":
  GDP = float(input("How much does GDP contributes to your happiness? (0-3): "))
  Health = float(input("How much does Life Expectancy contributes to your happiness? (0-1.15): "))
  CPI = float(input("How much does Corruption Perception Index contributes to your happiness? (0-91): "))
  Freedom = float(input("How much does your freedom perception contributes to your happiness? (0-0.73): "))
  Government_T = float(input("How much does Government Trust perception contributes to your happiness? (0-0.6): "))
  Social_s = float(input("How much does Social Support perception contributes to your happiness? (0-1.65): "))
  Dystopia_R = float(input("Based on the saddest country, how much happier you think ypu are? (0-3.6): "))

  # Create data frame with predictions
  numpy_array = np.array([[GDP, Health, CPI, Freedom, Government_T,Social_s, Dystopia_R]])
  new_df = pd.DataFrame(numpy_array, columns=['GDP','Health','CPI','Freedom', 'Government_T','Social_s','Dystopia_R'])
  prediction = RFregress.predict(new_df)
  rounded_pred = "{:.3f}".format(prediction[0])
  print("\n In a scale from 0-7, your happiness score is: ", rounded_pred)
  
  predict = input("\n Do you want to make another prediction? (Y/N): ")
  print("")

"""# PCA analysis"""

print(df.columns)
sns.lmplot(x='gdp_per_capita', y='happiness_score', data=df,fit_reg=False,legend=True,scatter_kws={"s":80})

df1 = df.drop(["health","Country","continent","Year","generosity","family","freedom","government_trust","dystopia_residual","social_support","cpi_score","cluster"], axis=1)
column_names = ["gdp_per_capita", "health", "cpi_score","freedom","government_trust","social_support","dystopia_residual"]    #New arrangement of classes
df1.head()
df1_new = pd.DataFrame([[rounded_pred,GDP]], columns=df1.columns).append(df1)
df1_new.head()

scaler = StandardScaler()
scaled_features1 = scaler.fit_transform(df1_new)

kmeans = KMeans(n_clusters=2)
clusters = kmeans.fit(df1_new)
#df['cluster'] = pd.Series(clusters.labels_, index=df.index)

kmeans = KMeans(
    n_clusters = 2,
    init="k-means++",
    n_init=20,
    max_iter=500,
    random_state=42 )

kmeans.fit(df1_new)
df_t1 = scaled_features1.T
plt.scatter(df_t1[0], df_t1[1], c=kmeans.labels_)

Cluster_p = kmeans.labels_[0]
print(Cluster_p)

if (Cluster_p == 0):
  print("You´re in group 1, that means your country may have some issues to solve and you can be a part of the change. Or just seek for better oportunities if worse")
else:
    print("You´re in group 2, that means you may have first world problems, such as having a mental breakdown when the internet is very slow")

df = pd.read_csv('Happiness_Score.csv')
df2 = df.drop(["happiness_score","Country","continent","Year","generosity","family"], axis=1)
df2.head()

pca = PCA(n_components=7)
pca.fit(df2)
df2_pca = pca.transform(df2)

df2_pca = pd.DataFrame(df2_pca)
df2_pca.index = df2.index
df2_pca.columns = ['PCA1','PCA2','PCA3','PCA4','PCA5','PCA6','PCA7']
df2_pca.head()

scaler = StandardScaler()
scaled_features = scaler.fit_transform(df2_pca)

df2_x = df[["gdp_per_capita", "health", "freedom", "government_trust", "dystopia_residual","social_support", "cpi_score"]]
df2_y = df[["happiness_score"]]

df2_pca.plot(
        kind='scatter',
        x='PCA2',y='PCA3',
        figsize=(16,8))

df2_pca2 = df2_pca.drop(['PCA1','PCA4','PCA5','PCA6','PCA7'], axis=1)
df2_pca2.head()

scaler = StandardScaler()
scaled_features2 = scaler.fit_transform(df2_pca2)

kmeans = KMeans(n_clusters=3)
clusters = kmeans.fit(df2_pca2)
df['cluster'] = pd.Series(clusters.labels_, index=df.index)

kmeans = KMeans(
    n_clusters = 3,
    init="k-means++",
    n_init=20,
    max_iter=500,
    random_state=42 )

kmeans.fit(df2_pca2)
df_t2 = scaled_features2.T
plt.scatter(df_t2[0], df_t2[1], c=kmeans.labels_)

kmeans = KMeans(n_clusters=7)
clusters = kmeans.fit(df2_pca)
df['cluster'] = pd.Series(clusters.labels_, index=df.index)

kmeans = KMeans(
    n_clusters = 7,
    init="k-means++",
    n_init=20,
    max_iter=500,
    random_state=42 )

kmeans.fit(df2_pca)
df_t = scaled_features.T
plt.scatter(df_t[0], df_t[1], c=kmeans.labels_)

from sklearn.neighbors import NearestNeighbors # importing the library
neighb = NearestNeighbors(n_neighbors=2) # creating an object of the NearestNeighbors class
nbrs=neighb.fit(df2_pca) # fitting the data to the object
distances,indices=nbrs.kneighbors(df2_pca) # finding the nearest neighbours

# Sort and plot the distances results
distances = np.sort(distances, axis = 0) # sorting the distances
distances = distances[:, 1] # taking the second column of the sorted distances
plt.rcParams['figure.figsize'] = (10,6) # setting the figure size
plt.plot(distances) # plotting the distances
plt.show() # showing the plot

# cluster the data into five clusters
dbscan = DBSCAN(eps = 1.09999, min_samples = 10).fit(df2_pca) # fitting the model
labels = dbscan.labels_ # getting the labels
# Plot the clusters
plt.scatter(df_t[0], df_t[1], c = labels) # plotting the clusters
#plt.xlabel("PCA2") # X-axis label
#plt.ylabel("PCA3") # Y-axis label
#plt.show() # showing the plot

kmeans.inertia_

NewCluster = dbscan.labels_[0]
print(NewCluster)